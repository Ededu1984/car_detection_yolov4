{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('yolo4')",
   "metadata": {
    "interpreter": {
     "hash": "d5be946e791667431ed25833de58b37858da2ea293530105219d846411feb9db"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Importando bibliotecas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "## Configurando caminhos e carregando arquivos do modelo treinado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = os.path.sep.join(['darknet/cfg/coco.names'])\n",
    "LABELS = open(labels_path).read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = os.path.sep.join(['darknet/yolov4.weights'])\n",
    "config_path = os.path.sep.join(['darknet/cfg/yolov4.cfg'])"
   ]
  },
  {
   "source": [
    "## Definindo configurações para a detecção "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(config_path, weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Todas as camadas (layers):\n['conv_0', 'bn_0', 'mish_1', 'conv_1', 'bn_1', 'mish_2', 'conv_2', 'bn_2', 'mish_3', 'identity_3', 'conv_4', 'bn_4', 'mish_5', 'conv_5', 'bn_5', 'mish_6', 'conv_6', 'bn_6', 'mish_7', 'shortcut_7', 'conv_8', 'bn_8', 'mish_9', 'concat_9', 'conv_10', 'bn_10', 'mish_11', 'conv_11', 'bn_11', 'mish_12', 'conv_12', 'bn_12', 'mish_13', 'identity_13', 'conv_14', 'bn_14', 'mish_15', 'conv_15', 'bn_15', 'mish_16', 'conv_16', 'bn_16', 'mish_17', 'shortcut_17', 'conv_18', 'bn_18', 'mish_19', 'conv_19', 'bn_19', 'mish_20', 'shortcut_20', 'conv_21', 'bn_21', 'mish_22', 'concat_22', 'conv_23', 'bn_23', 'mish_24', 'conv_24', 'bn_24', 'mish_25', 'conv_25', 'bn_25', 'mish_26', 'identity_26', 'conv_27', 'bn_27', 'mish_28', 'conv_28', 'bn_28', 'mish_29', 'conv_29', 'bn_29', 'mish_30', 'shortcut_30', 'conv_31', 'bn_31', 'mish_32', 'conv_32', 'bn_32', 'mish_33', 'shortcut_33', 'conv_34', 'bn_34', 'mish_35', 'conv_35', 'bn_35', 'mish_36', 'shortcut_36', 'conv_37', 'bn_37', 'mish_38', 'conv_38', 'bn_38', 'mish_39', 'shortcut_39', 'conv_40', 'bn_40', 'mish_41', 'conv_41', 'bn_41', 'mish_42', 'shortcut_42', 'conv_43', 'bn_43', 'mish_44', 'conv_44', 'bn_44', 'mish_45', 'shortcut_45', 'conv_46', 'bn_46', 'mish_47', 'conv_47', 'bn_47', 'mish_48', 'shortcut_48', 'conv_49', 'bn_49', 'mish_50', 'conv_50', 'bn_50', 'mish_51', 'shortcut_51', 'conv_52', 'bn_52', 'mish_53', 'concat_53', 'conv_54', 'bn_54', 'mish_55', 'conv_55', 'bn_55', 'mish_56', 'conv_56', 'bn_56', 'mish_57', 'identity_57', 'conv_58', 'bn_58', 'mish_59', 'conv_59', 'bn_59', 'mish_60', 'conv_60', 'bn_60', 'mish_61', 'shortcut_61', 'conv_62', 'bn_62', 'mish_63', 'conv_63', 'bn_63', 'mish_64', 'shortcut_64', 'conv_65', 'bn_65', 'mish_66', 'conv_66', 'bn_66', 'mish_67', 'shortcut_67', 'conv_68', 'bn_68', 'mish_69', 'conv_69', 'bn_69', 'mish_70', 'shortcut_70', 'conv_71', 'bn_71', 'mish_72', 'conv_72', 'bn_72', 'mish_73', 'shortcut_73', 'conv_74', 'bn_74', 'mish_75', 'conv_75', 'bn_75', 'mish_76', 'shortcut_76', 'conv_77', 'bn_77', 'mish_78', 'conv_78', 'bn_78', 'mish_79', 'shortcut_79', 'conv_80', 'bn_80', 'mish_81', 'conv_81', 'bn_81', 'mish_82', 'shortcut_82', 'conv_83', 'bn_83', 'mish_84', 'concat_84', 'conv_85', 'bn_85', 'mish_86', 'conv_86', 'bn_86', 'mish_87', 'conv_87', 'bn_87', 'mish_88', 'identity_88', 'conv_89', 'bn_89', 'mish_90', 'conv_90', 'bn_90', 'mish_91', 'conv_91', 'bn_91', 'mish_92', 'shortcut_92', 'conv_93', 'bn_93', 'mish_94', 'conv_94', 'bn_94', 'mish_95', 'shortcut_95', 'conv_96', 'bn_96', 'mish_97', 'conv_97', 'bn_97', 'mish_98', 'shortcut_98', 'conv_99', 'bn_99', 'mish_100', 'conv_100', 'bn_100', 'mish_101', 'shortcut_101', 'conv_102', 'bn_102', 'mish_103', 'concat_103', 'conv_104', 'bn_104', 'mish_105', 'conv_105', 'bn_105', 'leaky_106', 'conv_106', 'bn_106', 'leaky_107', 'conv_107', 'bn_107', 'leaky_108', 'pool_108', 'identity_109', 'pool_110', 'identity_111', 'pool_112', 'concat_113', 'conv_114', 'bn_114', 'leaky_115', 'conv_115', 'bn_115', 'leaky_116', 'conv_116', 'bn_116', 'leaky_117', 'conv_117', 'bn_117', 'leaky_118', 'upsample_118', 'identity_119', 'conv_120', 'bn_120', 'leaky_121', 'concat_121', 'conv_122', 'bn_122', 'leaky_123', 'conv_123', 'bn_123', 'leaky_124', 'conv_124', 'bn_124', 'leaky_125', 'conv_125', 'bn_125', 'leaky_126', 'conv_126', 'bn_126', 'leaky_127', 'conv_127', 'bn_127', 'leaky_128', 'upsample_128', 'identity_129', 'conv_130', 'bn_130', 'leaky_131', 'concat_131', 'conv_132', 'bn_132', 'leaky_133', 'conv_133', 'bn_133', 'leaky_134', 'conv_134', 'bn_134', 'leaky_135', 'conv_135', 'bn_135', 'leaky_136', 'conv_136', 'bn_136', 'leaky_137', 'conv_137', 'bn_137', 'leaky_138', 'conv_138', 'permute_139', 'yolo_139', 'identity_140', 'conv_141', 'bn_141', 'leaky_142', 'concat_142', 'conv_143', 'bn_143', 'leaky_144', 'conv_144', 'bn_144', 'leaky_145', 'conv_145', 'bn_145', 'leaky_146', 'conv_146', 'bn_146', 'leaky_147', 'conv_147', 'bn_147', 'leaky_148', 'conv_148', 'bn_148', 'leaky_149', 'conv_149', 'permute_150', 'yolo_150', 'identity_151', 'conv_152', 'bn_152', 'leaky_153', 'concat_153', 'conv_154', 'bn_154', 'leaky_155', 'conv_155', 'bn_155', 'leaky_156', 'conv_156', 'bn_156', 'leaky_157', 'conv_157', 'bn_157', 'leaky_158', 'conv_158', 'bn_158', 'leaky_159', 'conv_159', 'bn_159', 'leaky_160', 'conv_160', 'permute_161', 'yolo_161']\nTotal: 379\nCamadas de saída: \n[[327]\n [353]\n [379]]\n['yolo_139', 'yolo_150', 'yolo_161']\n"
     ]
    }
   ],
   "source": [
    "ln = net.getLayerNames()\n",
    "print(\"Todas as camadas (layers):\")\n",
    "print(ln)\n",
    "print(\"Total: \"+ str(len(ln)))\n",
    "print(\"Camadas de saída: \")\n",
    "print(net.getUnconnectedOutLayers())\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "print(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mostrar(img):\n",
    "  #fig = plt.gcf()\n",
    "  #fig.set_size_inches(16, 10)\n",
    "  #plt.axis(\"off\")\n",
    "  #plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "  #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blob_imagem(net, imagem, mostrar_texto=True):\n",
    "  inicio = time.time() \n",
    "  blob = cv2.dnn.blobFromImage(imagem, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "  net.setInput(blob)\n",
    "  layerOutputs = net.forward(ln)\n",
    "  termino = time.time()\n",
    "  if mostrar_texto:\n",
    "    print(\"YOLO levou {:.2f} segundos\".format(termino - inicio))\n",
    "  return net, imagem, layerOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deteccoes(detection, _threshold, caixas, confiancas, IDclasses):\n",
    "  scores = detection[5:] \n",
    "  classeID = np.argmax(scores)  \n",
    "  confianca = scores[classeID]\n",
    "\n",
    "  if confianca > _threshold:\n",
    "      caixa = detection[0:4] * np.array([W, H, W, H])     \n",
    "      (centerX, centerY, width, height) = caixa.astype(\"int\")\n",
    "            \n",
    "      x = int(centerX - (width / 2))\n",
    "      y = int(centerY - (height / 2))\n",
    "\n",
    "      caixas.append([x, y, int(width), int(height)])\n",
    "      confiancas.append(float(confianca))\n",
    "      IDclasses.append(classeID)\n",
    "      \n",
    "  return caixas, confiancas, IDclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcoes_imagem(imagem, i, confiancas, caixas, COLORS, LABELS, mostrar_texto=True):  \n",
    "  (x, y) = (caixas[i][0], caixas[i][1])\n",
    "  (w, h) = (caixas[i][2], caixas[i][3])\n",
    "\n",
    "  cor = [int(c) for c in COLORS[IDclasses[i]]]\n",
    "  cv2.rectangle(imagem, (x, y), (x + w, y + h), cor, 2) \n",
    "  texto = \"{}: {:.4f}\".format(LABELS[IDclasses[i]], confiancas[i])\n",
    "  if mostrar_texto:\n",
    "    print(\"> \" + texto)\n",
    "    print(x,y,w,h)\n",
    "  cv2.putText(imagem, texto, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, cor, 2)\n",
    "\n",
    "  return imagem,x,y,w,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pega_centro(x,y,w,h):\n",
    "    x1 = int(w/2)\n",
    "    y1 = int(h/2)\n",
    "    cx = x + x1\n",
    "    cy = y + y1\n",
    "    return cx, cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_linha = 600\n",
    "offset = 8"
   ]
  },
  {
   "source": [
    "___"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lendo arquivo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'video/Video1.mp4'\n",
    "cap = cv2.VideoCapture(video)\n",
    "conectado, video = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "conectado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1280, 720)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "video_largura = video.shape[1]\n",
    "video_altura = video.shape[0]\n",
    "video_largura, video_altura"
   ]
  },
  {
   "source": [
    "___"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Redimensionamento do tamanho do video (opcional)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redimensionar(largura, altura, largura_maxima = 600): \n",
    "  if (largura > largura_maxima):\n",
    "    proporcao = largura / altura\n",
    "    video_largura = largura_maxima\n",
    "    video_altura = int(video_largura / proporcao)\n",
    "  else:\n",
    "    video_largura = largura\n",
    "    video_altura = altura\n",
    "\n",
    "  return video_largura, video_altura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "threshold_NMS = 0.3\n",
    "fonte_pequena, fonte_media = 0.4, 0.6\n",
    "fonte = cv2.FONT_HERSHEY_SIMPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostras_exibir = 20\n",
    "amostra_atual = 0"
   ]
  },
  {
   "source": [
    "## Configurações de vídeo de saída e codec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_arquivo = 'resultado.avi'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') # MP4V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "saida_video = cv2.VideoWriter(nome_arquivo, fourcc, fps, (video_largura, video_altura))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "threshold_NMS = 0.3\n",
    "fonte_pequena, fonte_media = 0.4, 0.6\n",
    "fonte = cv2.FONT_HERSHEY_SIMPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "amostras_exibir = 20\n",
    "amostra_atual = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = 0"
   ]
  },
  {
   "source": [
    "## Processamento do vídeo e exibição do resultado"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "YOLO levou 2.35 segundos\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "imshow() missing required argument 'mat' (pos 2)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a1f140edb71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mamostra_atual\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mamostras_exibir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mamostra_atual\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: imshow() missing required argument 'mat' (pos 2)"
     ]
    }
   ],
   "source": [
    "while (cv2.waitKey(1) < 0):\n",
    "  conectado, frame = cap.read() # Lendo o frame do vídeo\n",
    "  if not conectado:\n",
    "    break\n",
    "  t = time.time()\n",
    "  frame = cv2.resize(frame, (video_largura, video_altura))\n",
    "  try:\n",
    "    (H, W) = frame.shape[:2]\n",
    "  except:\n",
    "    print('Erro')\n",
    "    continue\n",
    "\n",
    "  # Resultados da rede neural\n",
    "  imagem_cp = frame.copy() \n",
    "  net, frame, layerOutputs = blob_imagem(net, frame)\n",
    "  caixas = []       \n",
    "  confiancas = []   \n",
    "  IDclasses = []  \n",
    "  detec = []  \n",
    "\n",
    "  # Informações dos objetos detectados\n",
    "  for output in layerOutputs:\n",
    "    for detection in output:\n",
    "      caixas, confiancas, IDclasses = deteccoes(detection, threshold, caixas, confiancas, IDclasses)\n",
    "\n",
    "  objs = cv2.dnn.NMSBoxes(caixas, confiancas, threshold, threshold_NMS) # Reduzir bounding box mantendo somente o com maior probabilidade\n",
    "\n",
    "  # Destacando objetos através da criação dos retângulos e informações\n",
    "  if len(objs) > 0:\n",
    "    for i in objs.flatten():\n",
    "      frame, x, y, w, h = funcoes_imagem(frame, i, confiancas, caixas, COLORS, LABELS, mostrar_texto=False)\n",
    "      objeto = imagem_cp[y:y + h, x:x + w]\n",
    "\n",
    "      centro = pega_centro(x,y,w,h)\n",
    "      detec.append(centro)\n",
    "      cv2.circle(frame, centro, 4, (0,0,255), -1)\n",
    "\n",
    "      for (x,y) in detec:\n",
    "        if y<(pos_linha+offset) and y>(pos_linha-offset):\n",
    "          cars+=1\n",
    "          cv2.line(frame, (25, pos_linha), (1200, pos_linha), (0,127,255), 3)\n",
    "          detec.remove((x,y))\n",
    "          print(\"Car is detected: \"+str(cars))\n",
    "  \n",
    "  cv2.putText(frame, \" frame processado em {:.2f} segundos\".format(time.time() - t), \n",
    "              (20, video_altura-20), fonte, fonte_pequena, (250, 250, 250), 0, lineType=cv2.LINE_AA)\n",
    "\n",
    "  cv2.putText(frame, \"VEHICLE COUNTING: \"+str(cars), (450,70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 5)\n",
    "\n",
    "\n",
    "  \n",
    "  if amostra_atual <= amostras_exibir:\n",
    "    plt.imshow(frame)\n",
    "    amostra_atual += 1\n",
    "\n",
    "  saida_video.write(frame)\n",
    "\n",
    "print('Terminou')\n",
    "saida_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}